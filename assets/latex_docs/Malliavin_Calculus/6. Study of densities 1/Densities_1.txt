---
author:
- Liam Llamazares
bibliography:
- biblio.bib
date: 2022-10-24
title: Densities, existence, and regularity
---

::: CJK*
UTF8gbsn

#  Three line summary

-   A Malliavin differentiable random variable $X\in \mathbb{D}^{1,2}$
    is supported in intervals.

-   If $\|DX\|_{L^2(I)}$ is almost nowhere zero, then $X$ has a density
    $p$.

-   If $X$ is smooth then $p$ is smooth.

# Why should I care?

The density of a random variable (if it exists) gives us complete
information about it and is the fundamental goal of statistical
inference.

# Notation

Same as other posts, we recall in particular that $\mathbb{D}^\infty$ is
the space of Malliavin smooth random variables. Additionally, to
simplify many expressions, we introduce the notation $$H:=L^2(I).$$

# Introduction

Our goal is to establish some fundamental properties on the existence of
densities using Malliavin calculus. This is one of the first reasons why
Malliavin calculus was conceived. Firstly, we recall that given a random
variable $X:\Omega\to M$ valued in some metric space $M$, the support of
$X$ is defined to be the set of points $x\in M$ such that
$$\mathbb{P}_X(B_\epsilon(x))>0,\quad\forall \epsilon>0.$$ That is, $X$
has a positive probability of falling in any ball that contains $x$. Our
first result characterizes the support of real-valued Malliavin
differentiable random variables.

::: proposition
**Proposition 1**. *Let $X\in \mathbb{D}^{1,2}$, then the support of $X$
is an interval.*
:::

::: proof
*Proof.* Suppose not, then there must exist $a<x<b$ where $a,b$ are in
the support of $X$ and $x$ isn't. In particular, there exists
$\epsilon >0$ such that $$\label{sup}
            \mathcal{P}_X([x-\epsilon ,x+\epsilon ])=0.$$ Where we also
take $\epsilon$ small enough so that $a<x-\epsilon<x+\epsilon<b$. Now
take a smooth and bounded function $\varphi$ with $\varphi(y)=0$ for
$y<x-\epsilon$ and with $\varphi(y)=1$ for $y>x+\epsilon$. By
[\[sup\]](#sup){reference-type="eqref" reference="sup"} it holds that
(almost everywhere) $\varphi(X)=1_{X>x+\epsilon}$. Combining
proposition\... (link) and corollary (link) of the previous post, we
obtain that $1_{X>x+\epsilon}$ is either $0$ or $1$ almost everywhere.
That is,

$$\mathbb{P}(X>x+\epsilon)=0\quad \text{or}\quad \mathbb{P}(X\leq x+\epsilon)<0 .$$
But this contradicts that both $a,b$ are in the support of $X$. This
concludes the proof. ◻
:::

Ok, now we know the support of Malliavin differentiable functions are
nicely shaped intervals. But what about densities, when can we guarantee
their existence? Consider for example the simplest of all cases,
$X=x\in {\mathbb{R}}$. That is $X$ always takes the value $x$. Then $X$
is Malliavin differentiable, however, it admits no density (no, the
$\delta$ function does not count as a density). Let's consider a case
that is a bit less trivial, set
$$X=I_1(f)= \int_{I}f(t) dW(t),\quad f \in H.$$ Then, we have that $X$
is a Gaussian random variable with
$$X\sim \mathcal{N}(0,\norm{f}^2_{H}).$$ As a result, we obtain that $X$
will fail to have a density if (and only if)
$\norm{f}_{H}=\norm{DX}_{H}=0$. Notice that, in the previously
considered case where $X$ failed to have a density, we also had that
$\norm{DX}_{H}$ became $0$. The next proposition shows that this is not
a fluke.

::: theorem
**Theorem 1** (Existence of densities). *Let $X\in \mathbb{D}^{1,2}$. If
$\norm{DX}_{H}>0$ almost everywhere, then $X$ has a density.*
:::

::: proof
*Proof.* By the Radon Nikodyn theorem (link) all we need to do is show
that $\mathbb{P}_X$ is absolutely continuous with respect to the
Lebesgue measure $\lambda$. That is, we must show that if,
$\lambda (A)=0$ then also $\mathbb{P}_X(A)=0$. Let us set $g(y):=1_A(y)$
and $$\varphi(x):=\int_{-\infty}^x g(y)dy.$$ Then, by construction
$\varphi=0$, so also $\varphi(X)=0$. Suppose now that $g$ were
continuous. Then we would have that, by the fundamental theorem of
calculus $\varphi'(x)=g(x)$. So by the chain rule (link)
$$0=D \varphi(X)=\varphi'(X)DX=g(X)DX.$$ By taking $H$ norms on each
side, we would deduce that, since $DX$ is non-zero, $g(X)=1_A(X)$ must
be zero. That is, $\mathbb{P}_X(A)=0$, as desired. Of course, in
general, $g$ will not be differentiable (unless $A$ is actually $0$
everywhere). So to complete the proof, all that remains is to
approximate $g$ by continuous functions $g_n$ and take limits. See
[@nualart2018introduction] page 120 for the details. ◻
:::

The question is, can we give an explicit formula for the density of $X$.
The answer is (under some stricter conditions) yes, and the proof uses a
similar trick as our last one.

::: {#dens exists .theorem}
**Theorem 2** (Density expression). *Let $X\in \mathbb{D}^{1,2}$ such
that $\norm{DX}_{H}$ is non-zero almost everywhere. Suppose further that
$DX/\norm{DX}^2_{H}\in \text{dom} (\delta )$. Then $X$ has a density
given by
$$p(x):=\mathbb{E}\qty[1_{X\leq x}\delta \qty(\frac{DX}{\norm{DX}_{H}^2}) ].$$*
:::

::: proof
*Proof.* Let us consider $g\in C_c^\infty({\mathbb{R}})$ and set as
before $$\varphi(X):=\int_{-\infty}^x g(x)dx.$$ Then by the chain rule
(link) and the fundamental theorem of calculus, we have that
$$\left\langle D\varphi(X),\frac{DX}{\norm{DX}^2_{H}}\right\rangle_{H}=g(X).$$
By taking expectations on both sides, and using that the Skorohod
integral is the adjoint (link) of the Malliavin derivative, and Fubini
we obtain $$\begin{aligned}
             & \mathbb{E}[g(x)]=\mathbb{E}\qty[\left\langle D\varphi(X),\frac{DX}{\norm{DX}^2_{H}}\right\rangle_{H}]=\mathbb{E}\qty[\varphi(X)\delta\qty(\frac{DX}{\norm{DX}_{H}^2})]                                 \\
             & =\mathbb{E}\qty[\qty(\int_{{\mathbb{R}}}1_{X\leq x} g(x)dx)\delta\qty(\frac{DX}{\norm{DX}^2_{H}})]=\int_{{\mathbb{R}}}g(x)\mathbb{E}\qty[1_{X\leq x}\delta \qty(\frac{DX}{\norm{DX}_{H}^2}) ] \\&=\int_{{\mathbb{R}}}g(x)p(x).
        \end{aligned}$$ Since $g$ was any smooth function this concludes
the proof (as we can always approximate indicator functions
$1_A\in L^2({\mathbb{R}})$ by smooth functions). ◻
:::

To show the differentiability of $p(x)$ we need the following lemma
which will let us transfer- derivatives onto the density $p$.

::: lemma
**Lemma 1** (Integration by parts). *Let $X\in \mathbb{D}^{\infty}$ be
such that $\norm{DX}_{H}>0$ almost everywhere. Then, given any
$Y\in \mathbb{D}^\infty$ and $\varphi \in C_b^\infty({\mathbb{R}})$,
there exists $H_{k}(X,Y)\in \mathbb{D}^\infty$ such that
$$\mathbb{E}[\varphi^{(k)}(X)Y]=\mathbb{E}[\varphi(X)H_k(X,Y)].$$*

::: proof
**Proof.* We proceed by induction and consider first the case $k=1$. We
want to integrate parts in a probabilistic integral, as opposed to a
classical deterministic one, against the Lebesgue measure. The idea is
therefore to find a way to introduce the Malliavin differential and then
use the Skorohod integral to move derivates. To do so, we use one of our
previous tricks to write
$$\varphi'(X)=\left\langle D\varphi(X),\frac{DX}{\norm{DX}}_{H}\right\rangle_{H}.$$
By now taking expectations, moving $Y$ into the inner product on $H$,
and using that Skorohod integral is the adjoint of the Malliavin
derivative, we get that
$$\mathbb{E}[\varphi'(X)Y]=\mathbb{E}\qty[\varphi(X)\delta \qty(\frac{DX}{\norm{DX}}_{H}Y)]=:\mathbb{E}[\varphi(X)H_1(X,Y)].$$
Where $H_1(X,Y)\in \mathbb{D}^\infty$ because the Skorohod integral maps
$\mathbb{D}^\infty$ into $\mathbb{D}^\infty$. This resembles the fact
that the derivative maps the Schwartz space to itself. A proof of this
can be found in Hairer's online notes [@hairer2021introduction] page 15.
Now suppose that the lemma holds for $k=n$, then by the same procedure,
$$\begin{aligned}
                E[\varphi^{(n+1)}X] & =E[\varphi'(X) H_n(X,Y)]=\mathbb{E}[\varphi(X)H_1(X,H_n(X,Y))] \\&=:\mathbb{E}[\varphi(X)H_{n+1}(X,Y)].
            \end{aligned}$$ By the previous comment, since
$H_n(X,Y)\in \mathbb{D}^\infty$ also
$H_{n+1}(X,Y)\in \mathbb{D}^\infty$, which concludes the proof. ◻*
:::
:::

We now finally show that, if $X$ can be differentiated multiple times,
then so can its density.

::: theorem
**Theorem 3** (Smooth variables have smooth densities). *Let
$X\in \mathbb{D}^\infty$ such that $\norm{DX}_{H}>0$ almost everywhere,
then $X$ has a smooth density with respect to the Lebesgue measure.*
:::

::: proof
*Proof.* By Theorem [2](#dens exists){reference-type="ref"
reference="dens exists"} we know that $X$ has a density $p(x)$. To show
that $p$ is smooth, by Sobolev embedding (link) it is sufficient to show
that $p$ is weakly differentiable with order $n$ for all
$n \in \mathbb{N}$. That is, for all
$\varphi\in C_b^\infty({\mathbb{R}})$ there exists a function $p^{(n)}$
such that
$$\mathbb{E}[\varphi^{(n)}(X)]=\int_{{\mathbb{R}}}\varphi^{(n)}(x)p(x) dx=(-1)^n\int_{{\mathbb{R}}}\varphi(x) p^{(n)}(x) dx.$$
Where the first equality just holds by definition of density, so we only
have to prove the second. The idea will be to integrate by parts, using
the previous lemma to move derivatives off $\varphi$ and onto $1$ (as
strange as that may sound). We know that for any smooth
$\phi\in C_b^\infty({\mathbb{R}})$
$$\mathbb{E}[\phi^{(n+1)}(X)]=E[\phi'(X)H_n(X,1)].$$ If we apply this to
$\phi(x):=\int_{-\infty}^x \varphi(y) dy$ where $\varphi$ is any
function in $C_c^\infty({\mathbb{R}})$ (and in particular
$\varphi=\phi'$) we obtain that, by Fubini, $$\begin{gathered}
            \mathbb{E}[\varphi^{(n)}(X)]=E[\varphi(X)H_n(X,1)]= E\qty[\qty(\int_{{\mathbb{R}}}1_{[-\infty,X]}(x) \varphi(x)dx) H_n(X,1)]\\=\int_{{\mathbb{R}}}\varphi(x)\mathbb{E}\qty[1_{[-\infty,X]}(x)H_n(X,1)] dx=:(-1)^n \int_{R}\varphi(x) p^{(n)}(x) d
        \end{gathered}$$ This proves the theorem. ◻
:::

Finally, we comment that all these theorems carry over to the case where
$X$ is a vector-valued random variable (as opposed to real-valued). In
this case, the condition for the existence of densities becomes that the
matrix of derivatives $(\left\langle DX_i,DX_j\right\rangle_{H})_{i,j}$
has non-zero determinant almost everywhere. We refer the reader to
[@nualart2018introduction] (link) Chapter $7$ or
[@hairer2021introduction] chapter $4$ for the multidimensional theory.
:::
