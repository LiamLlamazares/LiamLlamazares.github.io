\documentclass[12pt]{article}
\special{papersize=3in,5in}
\usepackage[utf8]{inputenc}
%PACKAGES
\usepackage{CJKutf8}
\usepackage[colorlinks = true,
	linkcolor = blue,
	urlcolor  = black,
	citecolor = blue,
	anchorcolor = blue]{hyperref}
\usepackage[T1]{fontenc}
\makeatletter
\def\ps@pprintTitle{%
	\let\@oddhead\@empty
	\let\@evenhead\@empty
	\let\@oddfoot\@empty
	\let\@evenfoot\@oddfoot
}
\usepackage{amssymb,amsmath,physics,amsthm,xcolor,graphicx}
\usepackage[shortlabels]{enumitem}
\newtheorem{observation}{Observation}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newcommand{\red}[1]{{\color{red}#1}}
\usepackage[colorlinks = true,
	linkcolor = blue,
	urlcolor  = black,
	citecolor = blue,
	anchorcolor = blue]{hyperref}
\usepackage{cleveref}
\bibliographystyle{elsarticle-num}
\newcommand{\A}{\mathbb{A}}\newcommand{\C}{\mathbb{C}}\newcommand{\E}{\mathbb{E}}\newcommand{\F}{\mathbb{F}}\newcommand{\K}{\mathbb{K}}\newcommand{\LL}{\mathbb{L}}\newcommand{\M}{\mathbb{M}}\newcommand{\N}{\mathbb{N}}\newcommand{\PP}{\mathbb{P}}\newcommand{\Q}{\mathbb{Q}}\newcommand{\R}zzzzz\newcommand{\T}{{\mathbb T}}\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Aa}{\mathcal{A}}\newcommand{\Bb}{\mathcal{B}}\newcommand{\Cc}{\mathcal{C}}\newcommand{\Ee}{\mathcal{E}}\newcommand{\Ff}{\mathcal{F}}\newcommand{\Gg}{\mathcal{G}}\newcommand{\Hh}{\mathcal{H}}\newcommand{\Kk}{\mathcal{K}}\newcommand{\Ll}{\mathcal{L}}\newcommand{\Mm}{\mathcal{M}}\newcommand{\Nn}{\mathcal{N}}\newcommand{\Pp}{\mathcal{P}}\newcommand{\Qq}{\mathcal{Q}}\newcommand{\Rr}{{\mathcal R}}\newcommand{\Tt}{{\mathcal T}}\newcommand{\Zz}{{\mathcal Z}}\newcommand{\Uu}{{\mathcal U}}\newcommand{\Ss}{{\mathcal S}}
\newcommand\restr[2]{{\left.\kern-\nulldelimiterspace #1\vphantom{\big|} \right|_{#2}}}
\newcommand{\br}[1]{\left\langle#1\right\rangle}
\pagestyle{empty}
\setlength{\parindent}{0in}

\begin{document}
\title{Title}
\author{Liam Llamazares}
\date{date}
\maketitle
\section{ Three line summary}
\begin{itemize}
	\item
\end{itemize}
\section{Why should I care?}

\section{Notation}

\section{Introduction}
In previous posts we already used the Itô representation formula, which allows us to represent square integrable random variable $\xi $ of mean zero as an Itô integral of some stochastic process $X$. In general it is not possible to obtain an explicit expression for $X$, however in the case where $\xi $ is Malliavin differentiable we will obtain the expression $\E[D_t\xi |\Ff_t]$, that is,
\begin{theorem}[Clark-Ocone Formula]
	Given $\xi \in \mathbb{D}_{1,2}$ it holds that
	\begin{equation*}
		\xi =\E[\xi ]+\int_{I}\E[D_t\xi |\Ff_t] dW(t).
	\end{equation*}
\end{theorem}
To prove the following theorem we first have to take a brief detour to obtain some properties of the conditional expectation of (iterated) Itô integrals. First we give the following definition
\begin{definition}
	Let $B\in \Bb(I)$, we define $\Ff_G$ to be the completion of the $\sigma $-algebra generated by the Itô integrals
	\begin{equation*}
		\left\{dW(A):=\int_{A}dW(t): A\subset B, A \in \Bb(I)\right\}.
	\end{equation*}

\end{definition}
In practice one often takes $B=[0,t]$ in which case one obtains that $\Ff_B$ is the completion of the $\sigma $-algebra generated by $\{W(s): s<t\}$. That is,  $\Ff_B=\Ff_t$. We need the following results to prove the Clark-Ocone formula. Firstly, we have that the conditional expectation of the Itô integral of a deterministic function $f$ is just the Itô integral of $f$ on the smaller set. That is,
\begin{lemma}
	Let $f\in L^2(I,\Bb(I))$ and $B\in \Bb(I)$, then it holds that
	\begin{equation*}
		\E\qty[\int_{I}f(t)dW(t)|\Ff_B]=\int_{B} f(t) dW(t).
	\end{equation*}
\end{lemma}
\begin{proof}
	By definition of conditional expectation, we need to show that given a Borel set $A \subset B$ it holds that
	\begin{equation*}
		\E\qty[dW(A)\int_{I}f(t)dW(t)]=\E\qty[dW(A)\int_{B}f(t)dW(t)]
	\end{equation*}
	This follows from the Itô-isometry as both are equal to $\int_{A}f(t) d t$.
\end{proof}
\begin{observation}\label{obs}
	As a technicality we note that to complete the proof it is necessary to show that the right hand side of the equality of the lemma is indeed $\Ff_B$ measurable. This can be shown by assuming $f\in C_c(I)$ and then approximating the integral in the usual way as  a simple sum, that is
	$$\int_{B} f(t) dW(t)\sim \sum_{n=0}^{N}f(t_n)dW([t_{n},t_{n+1})\cap B)$$.
	Which is $\Ff_B$ measurable.
\end{observation}
Using this lemma we can prove a more general result when we now integrate a stochastic process  $X$.
\begin{lemma}\label{cond exp process}
	Let $X\in L^2(I\times\Omega,\Bb(I),\mathbb{F}_T)$ be a process adapted to $F_t$, then it holds that
	\begin{equation*}
		\E\qty[\int_{I}X(t)dW(t)|\Ff_B]=\int_{B} \E[X(t)|\Ff_B] dW(t).
	\end{equation*}
\end{lemma}
\begin{proof}
	The proof is analogous to that of the previous lemma. Furthermore, the justification of the $\Ff_B$ measurability of the right hand side is also carried out in the same fashion as in Observation \ref{obs}.
\end{proof}
Using the above lemma    may now prove the following way of taking conditional expectations in the Chaos expansion
\begin{lemma}
	Let $\xi \in L^2(\Omega,\Ff)$ have the chaos expansion $f=\sum_{n=0}^{\infty} I_n(f_n)$ and consider $B\in \Bb(I)$. Then it holds that
	\begin{equation*}
		\E[I_n(f_n)|\Ff_B]=I_n(1_B^{\otimes n}f_n).
	\end{equation*}
	Where $1_B^{\otimes n}(t_1,\ldots,t_n):=1_B(t_1),\cdot 1_B(t_n)$. As a result, we have that
	\begin{equation*}
		\E[\xi ]=\sum_{n=0}^{\infty}  f_n(x).
	\end{equation*}
\end{lemma}
\begin{proof}
	The proof proceeds by induction over $n$, the case $n=1$ holds by the previous Lemma \ref{cond exp process}. Suppose by hypothesis of induction that it holds for $n$, then we have that for  $n+1$
	\begin{align*}
		 & \E[I_{n+1}(f_{n+1})|\Ff_B]                                                                                                   \\
		 & =\int_{B}\E\qty[\int_{0}^{t_{n+1}}\cdot \int_{0}^{t_1} f_{n+1}(t_1,\ldots,t_{n+1})dW(t_1)\cdot  dW(t_n)\mid\Ff_G]dW(t_{n+1}) \\
		 & =\int_{B} I_n(1_B^{\otimes n}(\cdot )f_n(\cdot ,t_{n+1}))dW(t_{n+1})=I_n(1_B^{\otimes (n+1)}f_{n+1}).
	\end{align*}
	Where the first equality is due to Lemma \ref{cond exp process} and the second equality is a consequence of the hypothesis of induction. Finally the last past of the theorem follows by taking limits, as we recall that the chaos expansion converges in $L^2(\Omega)$ and thus also in $L^1(\Omega)$.
\end{proof}
Clark-Ocone's formula is now a straightforward calculation using the previous.
\begin{proof}Proof of Theorem 1: By, definition of the Malliavin derivative and the just proved Lemma it holds that
	\begin{align*}
		\int_{I}\E[D_tF|\Ff_t] dW(t) & =\sum_{n=1}^{\infty} n\int_{I} I_{n-1}(1_{[0,t]}^{\otimes(n-1)}f_{n-1}(\cdot ,t))dW(t)=\sum_{n=1}^{\infty} I_n(f_n) \\
		                             & =\xi -I_0(\xi )=\xi -\E[\xi ].
	\end{align*}


\end{proof}

\section{White noise analysis}
We recall that the space of tempered distributions $\Ss(\R^d\to \Cc^m)$ is the space of infinitely differentiable distributions that decrease faster than the inverse of any polynomial. That is,
\begin{equation*}
	\mathcal{S}(\R^d\to\C^m):=\lbrace f\in\C^\infty(\R^d\to\C^m):x^\alpha D^\beta f\in L^\infty(\R^d\to\C^m)\quad\forall\hspace{2pt}\alpha,\beta\in \Z^d\rbrace.
\end{equation*}
Furthermore, given a  vector space $V$ together with a countable family of semi-norms $\lbrace p_j\rbrace_{j=0}^\infty$ with the property that: given $x\neq 0$, there exists $j$ such that $p_j(x)\neq 0$. Then
\begin{equation}\label{seminormsgivemetric}
	d(x,y):=\sum_{j=0}^\infty 2^{-j}\frac{p_j(x-y)}{1+p_j(x-y)}\quad\forall{x,y}\in{X}
\end{equation}
is a translation invariant metric on $V$. We shall denote the dual of $V$  by \[V':=\lbrace w:V\to\C\hspace{2pt}: w \hspace{2pt}\text{continuous}\rbrace.\]
Due to the metric we set on $V$ one can verify that $w\in V'$ iff there exist $C\in\R,N\in\N$ such that
\begin{equation}\label{linearityFrechspace}
	\abs{w(u)}\leq C\sum_{j=1}^N p_j(u)\quad\forall{u}\in{V}
\end{equation}
In the case of the Schwartz space $\Ss(\R^d\to\C^m)$ we give it the topology induced by
\begin{equation}\label{seminornormsSchwartz}
	p_k(u):=\sum_{\abs{\alpha}\leq k} \sup_{x\in\R^d}\abs{x}^k \abs{D^\alpha u(x)}.
\end{equation}
Though, as is generally the case with Fréchet spaces, other families of semi-norms the reader may be familiar with such as
\[p_{k,\alpha}:=\sup_{x\in\R^d}\abs{x}^k \abs{D^\alpha u(x)};\quad\text{or}\quad p'_{k,\alpha}:=\sup_{x\in\R^d}(1+\abs{x})^k \abs{D^\alpha u(x)}\]
induce equivalent topologies. We note that with this metric $\Ss(\R^d\to\C^m)$ is a Fréchet space (that is a complete Hausdorff topological vector space). For a quick proof (in the case $m=1$) based on the fundamental theorem of calculus see for example \cite{Foll} page 237.
\bigbreak With this we move on to discuss space of tempered distributions.\\
\\
For our purposes it will suffice to consider the one-dimensional real case $\Ss(\R):=SS(\R\to\R)$. An equivalent way to generate the topology given by \eqref{seminormsgivemetric}-\eqref{seminornormsSchwartz} on $\Ss(\R)$ is to consider a sequence of decreasing  subspaces of $L^2(\R)$
\begin{equation*}
	L^2(\R):=\Ss_0(\R)\supset \Ss_1(\R)\cdots\supset \Ss_p(\R)\cdots.
\end{equation*}
Together with their norms $\norm{\cdot }_p$. Where these spaces and norms are taken such that
\begin{equation*}
	\Ss(\R)=\bigcap_{n \in \N}\Ss_p(\R),
\end{equation*}
, the topology generated by $\norm{\cdot }_p$ coincides with the previously defined one which was generated by $|\cdot |_p$, and the inclusion $\Ss_{p+1}(\R)\hookrightarrow \Ss_p(\R)$ is Hilbert Schmidt. This construction is very technical and can be found detailed in \cite{becnel2004schwartz}. The usage of this construction is that it forms part of a more general framework, which is as follows.
\begin{definition}
	We say that a real Hilbert space $\Nn$ is a nuclear space if there exist a decreasing family of subspaces $(H_p,\norm{\cdot }_p)$ such that
	\begin{equation*}
		\Nn(\R)=\bigcap_{n \in \N}H_p(\R).
	\end{equation*}
	Where the topology on $\Nn$ is generated by the family $\norm{\cdot }_p$, $\Nn$ is dense in $\Ss_p(\R)$,and for each $p \in \Nn$ there exists $q>p$ such that the inclusion  $H_q\hookrightarrow H_p$ is Hilbert Schmidt.
\end{definition}
By the previous discussion $\Ss(\R^d)$ is a nuclear space. Furthermore, we have the following,
\begin{theorem}[Bochner-Milos theorem]
	Let $\Nn$ be a nuclear space, then given a function $r:\Ss(\R)\to\C$ that is continuous and positive definite there exists a unique finite measure $\mu $ on $\Bb(\Nn)$ such that
	\begin{equation*}
		r(\phi)= \int_{\Nn} e^{i\br{\omega,\phi}}d \mu(\omega),\quad\forall \phi\in \Nn.
	\end{equation*}
\end{theorem}
We note that if $r(0)=1$ then  $\mu $ is a probability measure. When $r(\phi):=e^{-\frac{1}{2}\norm{\phi}_{H_0}^2}$ we call the probability space $(\Nn',\Bb(\Nn),\mu )$ the \emph{Gaussian space associated to $\Nn'$}. Particularizing this to $\Nn=\Ss(\R)$ we obtain the following classical space

\begin{definition}
	The \emph{white noise probability space} is the Gaussian space $(\Ss'(\R),\Bb(\Ss'(\R)),\mathbb{P} )$ associated to $\Ss'(\R)$. That is, $\mathbb{P} $ is a probability measure on $\Ss'(\R)$ such that
	\begin{equation}\label{characteristic func}
		\int_{\Ss(\R)} e^{i\br{\omega,\phi}}d \mathbb{P}(\omega)=e^{-\frac{1}{2}\norm{\phi}_{L^2(\R)}^2},\quad\forall \phi\in \S(\R).
	\end{equation}
\end{definition}
For a simpler and quick construction of this space without the explicit usage of the most general form of the Bochner-Milos theorem see also \cite{holden1996stochastic} appendix A.
We now make the following definition,
\begin{definition}
	\emph{Smoothed} white noise is the mapping $w:\Ss(\R)\times\Ss'(\R)\to \R$ defined by $\dot{W}(\phi,\omega):=\br{\omega,\phi}$.
\end{definition}
We note that, since $w$ is continuous it is measurable. Furthermore, if for each $\phi\in \Ss(\R)$ we consider the random variable   $\dot{W}(\phi):=\dot{W}(\phi,\cdot )$ on the white noise probability space $(\Ss'(\R),\Bb(\Ss'(\R)),\mathbb{P} )$ then, by basic theory of real valued Gaussian variables and \eqref{characteristic func}, $\dot{W}_\phi$ is a centered Gaussian variable with variance $\norm{\phi}_{L^2(\R)}$. In consequence we have an isometry
\begin{align*}
	(\Ss(\R),\norm{\cdot }  _{L^2(\R)} & )\to L^2(\Ss(\R),d\mathbb{P}) \\
	                                   & \phi\to \dot{W}_\phi.
\end{align*}
By density of $\Ss(\R)$ in $L^2(\R)$ the above isometry allows us to extend the definition of $\dot{W}(f)$ to all $f\in L^2(\R).$ Since the above mapping is an isometry is also conserves the inner product and we get the defining equality of white nois
\begin{equation*}
	\E[\dot{W}(f)\dot{W}(g)]=\br{f,g}_{L^2(\R)},\quad\forall f,g\in L^2(\R).
\end{equation*}
This shows that $\dot{W}$ is in fact a white noise on $L^2(\R)$. Another point of interest is that, if we write
\begin{equation}\label{iso}
	\int_{\R}f(t) dW(t):=\dot{W}(f),\quad\forall f\in L^2(\R).
\end{equation}
Then
\begin{equation*}
	W(t):=\int_{0}^t dW(t):= \int_{\R}1_{[0,t]} dW(t),
\end{equation*}
is in fact a Wiener process. The fact that $W(0)=0$ follows readily from the definition. Further,  $W(t)$ is a Gaussian random variable as it is defined as the limit in $L^2(\R)$ (and thus in distribution) of a sequence of Gaussian random variables $\dot{W}(\phi_n)$ where $\phi_n$ are Schwartz functions converging to $f$. The independence of $W(t)$ and  $W(s)$ is immediate by the isometry in \eqref{iso}. Finally, since $\dot{W}$ is a linear isometry, for any given $t\geq s$
\begin{equation*}
	\E[|W(t)-W(s)|^2]=\E[|\dot{W}(1_{[s,t]})|^2]=\norm{1_{[0,t-s]}}_{L^2(\R)}^2=t-s.
\end{equation*}
So by Kolmogorov's lemma, we make take a modification of $\dot{W}$  that  is $\gamma$ Hölder continuous for all $\gamma<\frac{1}{2}$. This concludes that indeed $W(t)$ is a Wiener process.\\\\
For completeness we note that the above construction may be replicated exactly to obtain a white noise on $L^2(\R^d)$. In this case the field on $\R^d$ defined by $W(x):=\dot{W}(1_{[0,x]})$ is called the Gaussian sheet.

\bibliography{biblio.bib}





\end{document}
