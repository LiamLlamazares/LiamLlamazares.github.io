\documentclass[11pt, a4paper, twoside]{book}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\bibliographystyle{elsarticle-num}

\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage[centering]{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathptmx}
\usepackage{mathrsfs}

\usepackage{mathtools}
\usepackage{textcomp}
\usepackage{xcolor}


\graphicspath{{figures/}}

\theoremstyle{plain}
\swapnumbers
\newtheorem{theorem}{Theorem}
\newtheorem{conjecturetheorem}[theorem]{
	This could be a theorem if I work hard enough
}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\lstset{%
	basicstyle=\small\ttfamily,
	breaklines=true,
	frame=leftline,
	framesep=6pt,
	numbers=left,
	showstringspaces=false
}

\pagestyle{fancy}

\renewcommand{\d}{\mathop{}\!\mathrm{d}}
\newcommand{\e}{\mathrm{e}}
\renewcommand{\i}{\mathrm{i}}
\renewcommand{\o}{\mathrm{o}}
\newcommand{\odv}[2]{\frac{\d{#1}}{\d{#2}}}
\newcommand{\pdv}[2]{\frac{\partial{#1}}{\partial{#2}}}

\def\Xint#1{\mathchoice
	{\XXint\displaystyle\textstyle{#1}}%
	{\XXint\textstyle\scriptstyle{#1}}%
	{\XXint\scriptstyle\scriptscriptstyle{#1}}%
	{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
	\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
			\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\newcommand{\bbone}{\mathbbm{1}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bfF}{\mathbf{F}}
\newcommand{\scrC}{\mathscr{C}}
\newcommand{\scrF}{\mathscr{F}}
\newcommand{\scrL}{\mathscr{L}}
\newcommand{\scrM}{\mathscr{M}}
\newcommand{\scrP}{\mathscr{P}}
\newcommand{\scrX}{\mathscr{X}}
\newcommand{\mres}{\mathbin{\vrule height 1.6ex depth 0pt width
		0.13ex\vrule height 0.13ex depth 0pt width 1.3ex}}

\newcommand{\DeltaOrd}{\Delta_\mathrm{ord}^{N-1}}

\newcommand{\subsolution}{}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\angles}{\langle}{\rangle}
\DeclarePairedDelimiter{\eval}{.}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\parens}{\lparen}{\rparen}
\DeclarePairedDelimiter{\brackets}{\lbrack}{\rbrack}
\DeclarePairedDelimiter{\set}{\{}{\}}

\DeclareMathOperator\diverg{div}
\DeclareMathOperator\supp{supp}
\DeclareMathOperator\id{id}

\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\title{The Malliavin derivative, part 2}

\fancyhead[LE]{Malliavin calculus}
\fancyhead[RO]{\nouppercase{\rightmark}}
\fancyfoot[C]{}
\fancyfoot[R]{\thepage}

\renewcommand{\thesection}{\arabic{section}}

\newenvironment{reportday}[1]
{%
	\hline\vspace{0.5em}
	\noindent{\bf #1}%
}
{%
	\vspace{0.5em}\hline\vspace{0.5em}
	\normalfont
}

\newenvironment{solution}
{%
	\newcounter{subsolutioncounter}
	\renewcommand{\subsolution}[1][(\alph{subsolutioncounter})]
	{%
		\stepcounter{subsolutioncounter}
		\vspace{0.5em}
		{\it {##1} \,}
	}
	\noindent{\it Solution}%
}
{%
	\hfill \Diamond
	\vspace{0.5em}\hline\vspace{0.5em}
}


\makeatletter
\newenvironment{subxarray}{%
	\vcenter\bgroup
	\Let@ \restore@math@cr \default@tag
	\baselineskip\fontdimen10 \scriptfont\tw@
	\advance\baselineskip\fontdimen12 \scriptfont\tw@
	\lineskip\thr@@\fontdimen8 \scriptfont\thr@@
	\lineskiplimit\lineskip
	\ialign\bgroup\hfil
	$\m@th\scriptstyle##$&$\m@th\scriptstyle{}##$\hfil\crcr
}{%
	\crcr\egroup\egroup
}
\makeatother
\author{Billy Summers}

\begin{document}
\maketitle

\section{The Malliavin derivative as a Fr\'{e}chet derivative}
Let $C_0([0, T])$ be the Banach space of all continuous functions $f \colon [0,
		T] \to \bbR$ such that $f(0) = 0$. On this space, we can associate a special
Borel probability measure $\mu$ such that $W_t(\omega) := \omega(t)$ is a
Brownian motion. Given a random variable on $C_0([0, T])$, i.e. a
$\mu$-measurable function $X \colon C_0([0, T]) \to \bbR$, we want to know how
the value $X(\omega)$ changes upon perturbing the path $\omega$ by a small
quantity $\gamma \in C_0([0, T])$. This can be described by the Fr\'{e}chet
derivative $\nabla{X}(\omega)$, which is a bounded linear map $C_0([0, T]) \to
	\bbR$, i.e. a member of the dual space $C_0([0, T])^*$, giving the best linear
approximation to the difference $X(\omega + \gamma) - X(\omega)$. Formally,
$\nabla{X}$ satisfies
\[
	X(\omega + \gamma)
	= X(\omega)
	+ \angles{\nabla{X}(\omega), \gamma}
	+ \o(\norm{\gamma}_{C_0([0, T])}).
\]
If $X$ has a Fr\'{e}chet derivative $\nabla{X} \colon C_0([0, T]) \to C_0([0,
			T])^*$, we say it is \emph{Fr\'{e}chet differentiable}.

Within the Banach space $C_0([0, T])$ lies a Hilbert space $H$ of distinguished
elements. This is the space of paths of the form
\[
	\gamma(t) = \int_0^t \psi(s) \, \d{s}
\]
for some $\psi \in L^2([0, T])$. In other words, it is the space of $W^{1, 2}$
functions on $[0, T]$ starting at 0. Its inner product is given by
\[
	(\gamma_1, \gamma_2)_H = (\dot{\gamma}_1, \dot{\gamma}_2)_{L^2([0, T])}.
\]
$H$ is continuously imbedded in $C_0$ by the theory of Sobolev spaces. We call
$H$ the \emph{Cameron-Martin space}. It acts in some sense as the heart
of $C_0([0, T])$ with the probability measure $\mu$, with its elements having
better analytical properties compared to a general element in $C_0([0, T])$.
Ideally, we could restrict $\mu$ to this space and only work here, but
unfortunately $\mu$ is not a measure on $H$ (in particular, it is not
$\sigma$-additive), forcing us to work in a larger Banach space.

Returning to our Fr\'{e}chet differentiable random variable $X$, given some
path $\omega \in C_0([0, T])$, we consider the restriction of its derivative at
$\omega$ to $H$, namely $\eval*{\nabla{X}(\omega)}_H$. Since $H$ is continously
imbedded in $C_0$, this restriction is an element of $H^*$. Then, as $H$ is a
Hilbert space, the dual space $H^*$ is isomorphic to $H$ through its inner
product, so there exists some $DX(\omega) \in L^2([0, T])$ such
that
\[
	\angles*{\nabla{X}(\omega), \int_0^\cdot \psi \, \d{t}}
	= \parens*{\int_0^\cdot D_t{X}(\omega) \, \d{t},
	\int_0^\cdot \psi \, \d{t}}_H
	= \int_0^T D_t{X}(\omega) \psi(t) \, \d{t}.
\]
This object $DX$ is precisely the Malliavin derivative of $X$.

\section{The Skorokhod integral and the Malliavin derivative}
Given a stochastic process $(X_t)_{t \in [0, T]} \in L^2([0,T]\times\Omega,
	\d{t}\otimes\bbP)$ such that $X_t$ is $\scrF_T$-measurable for all $t \in
	[0, T]$, let
\[
	X_t = \sum_{n = 0}^\infty I_n(f_n(\cdot, t))
\]
be its chaos expansion for some $f_n \in L^2([0, T]^{n + 1})$ symmetric in the
first $n$ variables. Recall we say $X$ is \emph{Skorokhod integrable}, and
define its Skorokhod integral by
\[
	\int_0^T X_t \, \delta{W}_t := \sum_{n = 0}^\infty I_{n + 1}({f}_{n,S}),
\]
whenever this sum converges in $L^2(\Omega)$. The following result is
fundamental.

\begin{theorem}
	The Skorokhod integral and Malliavin derivative are \emph{adjoint} in the
	following sense:

	Let $(X_t)_{t \in [0, T]}$ be a Skorokhod-integrable. Let $Y \in \bbD^{1,
			2}$ be a Malliavin differentiable random variable. Then
	\[
		\parens*{
			Y, \int_0^T X_t \, \delta{W}_t
		}_{L^2(\Omega)}
		= \parens*{
			D{Y}, X
		}_{L^2([0,T]\times\Omega)}.
	\]
	More concretely,
	\[
		\bbE\brackets*{
			Y \int_0^T X_t \, \delta{W}_t
		}
		= \bbE\brackets*{
			\int_0^T
			D_t Y X_t
			\, \d{t}
		}.
	\]
\end{theorem}
\begin{proof}
	As usual, we apply the definitions in terms of the chaos expansions. Let
	\[
		X_t = \sum_{n = 0}^\infty I_n(f_n(\cdot, t))
	\]
	be the chaos expansion of $X$, and
	\[
		Y = \sum_{n = 0}^\infty I_n(g_n)
	\]
	the chaos expansion of $Y$. Then
	\begin{align*}
		\bbE\brackets*{
			Y \int_0^T X_t \, \delta{W}_t
		}
		 & = \bbE\brackets*{
			\sum_{n = 0}^\infty I_n(g_n)
			\sum_{m = 0}^\infty I_{m + 1}({f}_{m,S})
		}                                                                 \\
		 & = \sum_{n = 0}^\infty \sum_{m = 0}^\infty
		\bbE[I_n(g_n) I_{m + 1}({f}_{m,S})]                               \\
		 & = \sum_{n = 0}^\infty n! (g_n, {f}_{n - 1,S})_{L^2([0, T]^n)},
	\end{align*}
	and on the other side,
	\begin{align*}
		\bbE\brackets*{
		\int_0^T D_t{Y} X_t \, \d{t}
		}
		 & = \int_0^T \bbE\brackets*{
			\sum_{n = 1}^\infty n I_{n - 1}(g_n(\cdot, t))
			\sum_{m = 0}^\infty I_m(f_m(\cdot, t))
		} \, \d{t}                                            \\
		 & = \int_0^T \sum_{n = 0}^\infty \sum_{m = 0}^\infty
		n \bbE\brackets{
			I_{n - 1}(g_n(\cdot, t)) I_m(f_m(\cdot, t))
		} \, \d{t}                                            \\
		 & = \sum_{n = 0}^\infty
		n (n - 1)! \int_0^T
		(g_n(\cdot, t), f_{n - 1}(\cdot, t))_{L^2([0, T]^{n - 1})}
		\, \d{t}                                              \\
		 & = \sum_{n = 0}^\infty
		n! (g_n, f_{n - 1})_{L^2([0, T]^n)}.
	\end{align*}
	Finally, by definition of the symmetrization,
	\begin{align*}
		(g_n, f_{n - 1,S})_{L^2([0, T]^n)}
		 & = \int_{[0, T]^n}
		g_n(t_1, \dots, t_n)
		\frac{1}{n} \sum_{k = 1}^n
		f(t_1, \dots, t_{k - 1}, t_n, t_{k + 1}, \dots, t_{n - 1}, t_k)
		\, \d{t_1} \cdots \d{t_n}                       \\
		 & = \frac{1}{n} \sum_{k = 0}^n \int_{[0, T]^n}
		g_n(t_1, \dots, t_n)
		f_{n - 1}(t_1, \dots, t_n)
		\, \d{t_1} \cdots \d{t_n}                       \\
		 & = (g_n, f_{n - 1})_{L^2([0, T]^n)},
	\end{align*}
	where we change variables $t_k \mapsto t_n, t_n \mapsto t_k$, use the
	property that $g_n$ is symmetric, and apply Fubini's theorem.
\end{proof}

\begin{remark}
	The symbol $\delta$ is often used for a divergence-like operator in Hodge
	theory. The analogy with our case is that in the Hodge situation, $\delta$
	is defined via a duality formula which looks like $\angles{d{\alpha},
			\beta} = \angles{\alpha, \delta{\beta}}$, where $d$ is the exterior
	derivative on differential forms. Indeed, even in vector calculus, the
	negative of the divergence is in some sense adjoint to the gradient:
	\[
		\int_\Omega \diverg{f} \phi \, \d{x}
		= - \int_\Omega f \cdot \nabla{\phi} \, \d{x}
	\]
	whenever $\phi$ has zero boundary. So, in a sense, the Skorokhod integral is
	just a divergence operator.
\end{remark}

Using this, we can immediately prove the following:

\begin{corollary}
	Let $(X^n)_{n \in \bbN}$ be a sequence of Skorokhod-integrable stochastic
	processes. Suppose there exist $X \in L^2([0,T]\times\Omega)$ and $Y
		\in L^2(\Omega)$ such that $X^N \to X$ in $L^2([0,T]\times\Omega)$, and
	$\delta{X^N} \to Y$ in $L^2(\Omega)$. Then $X$ is Skorokhod integrable, and
	$\delta{X^N} \to \delta{X}$.
\end{corollary}
\begin{proof}
	Recall that Skorokhod integrability of $X$ can be expressed in terms of
	convergence of the series
	\[
		\sum_{n = 0}^\infty
		(n + 1)! \norm{{f}_{n,S}}_{L^2([0, T]^{n + 1})}^2,
	\]
	where $f_n(\cdot, t)$ are the components of the chaos expansion of $X$.
	Since $X^N \to X$ strongly in $L^2$, and each $X^N$ is Skorokohod
	integrable, the components of their chaos expansions must satisfy the above
	condition, and we can take limits.

	Let $Z \in \bbD^{1, 2}$. Then by adjointness,
	\[
		(Z, \delta{X^N})_{L^2(\Omega)}
		= (DZ, X^N)_{L^2([0,T]\times\Omega)}.
	\]
	Taking limits on both sides and using adjointness on the limiting
	objects gives us
	\[
		(Z, Y)_{L^2(\Omega)}
		= (DZ, X)_{L^2([0,T]\times\Omega)}
		= (Z, \delta{X})_{L^2(\Omega)}.
	\]
	Then, since $\bbD^{1, 2}$ is dense in $L^2(\Omega)$, we see that $Y =
		\delta{X}$ a.s., as required.
\end{proof}

\begin{remark}
	Perhaps a more intuitive way to say the Skorokhod integral is ``closable''
	in the book's words is that it is sequentially continuous as a map
	$D(\delta) \subseteq L^2([0,T]\times\Omega) \to L^2(\Omega)$ with
	respect to the strong $L^2$ topology in its domain and weak $L^2$ topology
	in its codomain.
\end{remark}

\begin{theorem}
	Let $X \in L^2([0,T]\times\Omega)$ be a Skorokhod integrable random
	process, and let $Y \in \bbD^{1, 2}$ be such that $FX$ is also Skorokhod
	integrable. Then
	\[
		Y \int_0^T X_t \, \delta{W}_t
		= \int_0^T Y X_t \, \delta{W}_t + \int_0^T D_t Y X_t \, \d{t}
	\]
	almost surely.
\end{theorem}
\begin{proof}
	Suppose $Y$ has finite chaos expansion, and choose some $Z \in \bbD^{1, 2}$
	also with finite chaos expansion. Then by adjointness and the product rule,
	\begin{align*}
		\bbE\brackets*{
			Z \int_0^T Y X_t \, \delta{W}_t
		}
		 & = \bbE\brackets*{
		\int_0^T D_t{Z} Y X_t \, \d{t}
		}                    \\
		 & = \bbE\brackets*{
		\int_0^T (D_t(YZ) - Z D_t{Y}) X_t \, \d{t}
		}                    \\
		 & = \bbE\brackets*{
			YZ \int_0^T X_t \, \delta{W}_t
		} - \bbE\brackets*{
		Z \int_0^T D_t{Y} X_t \, \d{t}
		}.
	\end{align*}
	Since the set of all test functions $Z \in \bbD^{1, 2}$ with finite chaos
	expansion is dense in $L^2(\Omega)$, we conclude the result for $Y$ with
	finite chaos expansion. For general $Y$, we approximate.
\end{proof}

\begin{remark}
	A similar formula crops up in vector calculus, namely the following:
	\[
		\diverg(fX) = \nabla{f} \cdot X + f \diverg{X},
	\]
	where $f$ is a scalar function and $X$ a vector field. Again, in the above
	theorem, the Malliavin derivative takes the place of the gradient, the
	Skorokhod integral take the place of the divergence, and the usual inner
	product on $\bbR^n$ (the dot product) is replaced with the $L^2([0, T])$
	inner product. There is a sign difference owing to the fact the adjointness
	in the Malliavin case does not induce a sign change, unlike in the vector
	calculus case (see the remark above).
\end{remark}

\begin{theorem}
	Let $X \in L^2([0,T]\times\Omega)$ be a stochastic process such that
	for all $s \in [0, T]$, $X_s$ is in $\bbD^{1, 2}$, $D{X_s}$ is
	Skorokhod integrable, and
	\[
		\int_0^T DX_s \, \delta{W}_s \in L^2([0,T]\times\Omega).
	\]
	Then $\delta{X}$ lies in $\bbD^{1, 2}$, and
	\[
		D_t(\delta{X}) = \int_0^T D_t{X_s} \, \delta{W}_s + X_t.
	\]
\end{theorem}

\begin{remark}
	The technical constraints in the theorem above are an unfortunate
	consequence of the fact the Skorokhod and Malliavin operators $\delta$ and
	$D$ are not defined on the full space $L^2([0,T]\times\Omega)$ and
	$L^2(\Omega)$ respectively - we have to ensure an operator throws us to the
	right spot before we can consider applying the other one.

	Note that this theorem is simply an expression of the Malliavin derivative
	and Skorokhod integral's failure to commute, with the error simply being
	the identity on $L^2(\Omega \times [0, 1])$. That is,
	\[
		D\delta = \delta D + \id.
	\]
	This contrasts with our vector
	calculus analogy, where the divergence and gradient most certainly commute
	(assuming enough regularity).
\end{remark}
\end{document}
